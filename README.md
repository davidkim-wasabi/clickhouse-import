# ClickHouse Import
Scripts related to importing data into ClickHouse, initializing new tables, filtering nulls, and so forth.

To set the credentials for ClickHouse, create a file `clickhouse_secrets.py` and set `user` and `password` variables appropriately.

Example usage: 
```python
os.system('tail -n +2 TaxJarTaxCalc.csv | \
docker exec -i clickhouse-server clickhouse-client --user {} --password {} \
--query="INSERT INTO BA_Billing.TaxJarTaxCalc FORMAT CSV"'.format(clickhouse_secrets.user, clickhouse_secrets.password))
```

## Usage
Some of these scripts were developed by Amos Kendall (the other intern), but I will explain the ones I worked on:

- `filter_null_strings.py` will go through the ClickHouse database, check if any entries are "null-like" but not actually null (e.g. the String value "Null"), then report back which tables are the offenders. It is currently configured to automatically change the schemas of those tables to have those affected columns become Nullable. `json_serializer.py` is used in tandem to serialize the output nicely to `null_counts.json` and `null_counts.txt`.
- `loadDB_bulk.py` loads all the CSVs into ClickHouse from a specific path. The path is a bit weird: `../mysql_data_transfer/BA_Billing` and `../mysql_data_transfer/BA_Global`. These are generated by the other repo's scripts. The readme there should clarify any questions you may have.
- `createDB_bulk.py` will mass-create tables in ClickHouse as specified by the dbstarter files in `../mysql_data_transfer/GDB_dbstarter`. Caution is advised as they will drop tables if they already exist, without prompt.
